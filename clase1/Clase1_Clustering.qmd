---
title: "Clustering"
format: html
editor: visual
---

# Prerequisitos
Las siguientes notas se basan en las desarrolladas en a√±os anteriores por Eduardo Castro.
-   Anaconda (2024.02-1)
-   R (4.0.0) y Rstudio (2024.04.1+748 \| Released: 2024-05-11)
-   Quarto
-   Visual Studio code con complmentos de Jupyter Notebook
-   Los siguientes paquetes de R se requieren:

```{r, warning=FALSE}
library(dplyr)
library(ggplot2)
library(patchwork)
library(reticulate)
library(cluster)
library(factoextra) # M√©todo silhoute
library(dbscan) # Para m√©todo alternativo de k-means
library(klaR) # Para k mediods

```

# Metodolog√≠as para n√∫mero de clusters

```{r}
iris_data <- iris[,1:2]
head(iris_data)
```

Graficamos esta nube de puntos:

```{r}
ggplot(data = iris_data, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() + theme_bw()
```

## M√©todo Silhoute

El algoritmo en R realiza lo siguiente para calcular el coeficiente de silhouette:

-   Para cada observaci√≥n en el conjunto de datos, se calcula la distancia media al resto de observaciones dentro de su propio cl√∫ster (a) y la distancia media al resto de observaciones en el cl√∫ster m√°s cercano diferente (b).
-   El coeficiente de silhouette para cada observaci√≥n se calcula como (b - a) / max(a, b).
-   El coeficiente de silhouette promedio para el conjunto de datos se calcula como el promedio de los coeficientes de silhouette individuales.

Con este coeficiente podemos encontrar el n√∫mero √≥ptimo de clusters:

```{r}
fviz_nbclust(iris_data,kmeans, method = "silhouette",k.max=20)
```

## M√©todo de distancia cuadrada dentro

La suma de los cuadrados dentro del cl√∫ster (WSS) es una medida de cu√°n cohesivos son los cl√∫steres, es decir, cu√°n cerca est√°n los puntos dentro de un cl√∫ster entre s√≠. El m√©todo de codo consiste en calcular la WSS para diferentes valores de k (n√∫mero de cl√∫steres) y graficar estos valores en funci√≥n de k. La idea es identificar el "codo" en la curva, que es el punto donde la WSS comienza a disminuir de manera m√°s lenta. Este punto indica el n√∫mero √≥ptimo de cl√∫steres, ya que agregar m√°s cl√∫steres a partir de este punto no proporciona una mejora significativa en la cohesi√≥n dentro de los cl√∫steres.

Las f√≥rmulas para calcular la WSS son las siguientes:

Para cada cl√∫ster $C_i$

Se calcula la distancia cuadrada de cada punto al centroide del cl√∫ster $\mu_i$

Se suman estas distancias cuadradas para obtener la WSS del cl√∫ster $WSS_i$

$$WSS_i = \sum_{x \in C_i} (x - \mu_i )^2$$
La suma de cuadrados dentro:

```{r}
fviz_nbclust(iris_data,kmeans, method = "wss",k.max=20)

```

## Estad√≠stica de brecha

El m√©todo de estad√≠sticas de brecha compara la dispersi√≥n intra-cl√∫ster de un conjunto de datos con la que se esperar√≠a si los datos fueran generados aleatoriamente sin estructura de cl√∫steres. La idea es que si los datos tienen una estructura de cl√∫steres significativa, la dispersi√≥n intra-cl√∫ster ser√° mayor que en datos aleatorios.

El m√©todo de estad√≠sticas de brecha se calcula en tres pasos:

- Se ajustan modelos de clustering para diferentes valores de k (n√∫mero de cl√∫steres).
- Se calcula la estad√≠stica de brecha para cada valor de ùëò
k, que mide la diferencia entre la dispersi√≥n intra-cl√∫ster observada y la esperada bajo una referencia nula (datos aleatorios).
- Se selecciona el valor deùëòk que maximiza la estad√≠stica de brecha.


Dispersi√≥n Intra-Cl√∫ster Observada ($W_k$: Se calcula la suma de los cuadrados dentro del cl√∫ster para el valor de k. Esta medida representa qu√© tan compactos son los cl√∫steres en los datos reales.

Dispersi√≥n Intra-Cl√∫ster Esperada ($W_{kb}$): Se calcula la dispersi√≥n intra-cl√∫ster para un conjunto de datos aleatorios con el mismo n√∫mero de puntos y la misma distribuci√≥n de caracter√≠sticas que los datos originales. Este c√°lculo se realiza promediando la dispersi√≥n intra-cl√∫ster sobre un cierto n√∫mero de iteraciones.

Estad√≠stica de Brecha ($GAP_k$): La estad√≠stica de brecha para el valor de k dado se calcula como la diferencia entre la dispersi√≥n intra-cl√∫ster observada y la esperada bajo la referencia nula:

$$GAP_k = \frac{1}{B}\sum_{b=1}^{B}log(W_{kb})-log(W_k)$$

```{r}
fviz_nbclust(iris_data,kmeans, method = "gap_stat",k.max=20)
```

# M√©todos alternativos de clustering 


No todos los problemas se pueden resolver con kmeans, sino que existen t√©cnicas m√°s apropiadas para detectar de forma m√°s efectiva como se compartan los puntos en un conjunto de datos:

```{r}
df_shapes <- read.csv("multishapes.csv") %>% dplyr::select(x, y)
```

Calculamos por k - meadias:

```{r}
set.seed(12345)
kmedias <- kmeans(df_shapes, 5)
df_shapes$grupo_kmedias <- kmedias$cluster %>% as.character()
```


## DBSCAN  (Density-Based Spatial Clustering of Applications with Noise)

AHora calculamos por el m√©todo debscan:

```{r}
dbscan_method <- dbscan(df_shapes %>% dplyr::select(x, y), eps = .15, minPts = 5)
df_shapes$grupo_dmscan <- dbscan_method$cluster %>% as.character()
```

```{r}
table(df_shapes$grupo_dmscan)
```

```{r}

grafico_kmeans <- ggplot(data = df_shapes, aes(x, y, color = grupo_kmedias)) + geom_point()
grafico_dmscan <- ggplot(data = df_shapes, aes(x, y, color = grupo_dmscan)) + geom_point()
grafico_kmeans + grafico_dmscan
```

En Python podemos resolver de la siguiente manera:

Cargamos los paquetes:

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.cluster import DBSCAN
from collections import Counter
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
```

Traemos unos datos para desarrollar elm√©todo debscan en Python:

```{python}

# Read the CSV file
df_shapes = pd.read_csv("multishapes.csv")

# Select columns 'x' and 'y'
df_shapes = df_shapes[['x', 'y']]
```

Calculamos el m√©todo dbscan:

```{python}
dbscan_method = DBSCAN(eps = 0.15, min_samples = 5, metric = 'euclidean')
plt.scatter(df_shapes.iloc[:,0], df_shapes.iloc[:,1], c = dbscan_method.fit_predict(df_shapes))
plt.title('Toy Problem with Minimum Points: ' + str(5))
plt.show()
```

EL -1 hace referencia al grupo de outliers:

```{python}
labels = dbscan_method.fit_predict(df_shapes)
Counter(labels)
```

Ilustraremos el m√©todo para diferentes valores de epsilon:

```{python}
XX, yy = make_blobs(n_samples=1000, centers=8, n_features=2, random_state=800)
# Visualize the data
plt.scatter(XX[:,0], XX[:,1])
plt.show()
```

```{python, warning = FALSE, message = FALSE}
num_samples = [10,19,20]

for min_num in num_samples:
    db = DBSCAN(eps=0.7, min_samples=min_num, metric='euclidean')
    plt.scatter(XX[:,0], XX[:,1], c=db.fit_predict(XX))
    plt.title('Toy Problem with Minimum Points: ' + str(min_num))
    plt.show()
```

Observemos como var√≠a el radio:

```{python}
epsilons = [0.4,0.7]

for epsilon in epsilons:
    db = DBSCAN(eps=epsilon, min_samples=10, metric='euclidean')
    plt.scatter(XX[:,0], XX[:,1], c=db.fit_predict(XX))
    plt.title('Toy Problem with Minimum Points: ' + str(epsilon))
    plt.show()
```

Podemos tambi√©n entrenar el modelo para cuando lleguen nuevos datos:

```{python}
# Assuming your data has features 'x' and 'y', select those columns

# Split the data into training and testing sets
X_train, X_test = train_test_split(df_shapes, test_size=0.2, random_state=12345)

# Initialize DBSCAN model
dbscan = DBSCAN(eps=0.15, min_samples=5)

# Train the model
dbscan.fit(X_train)

# Predict cluster labels for the test data
test_labels = dbscan.fit_predict(X_test)
print(test_labels)
```

```{python}
plt.scatter(X_test.iloc[:,0], X_test.iloc[:,1],
            c = dbscan_method.fit_predict(X_test))
plt.title('Toy Problem with Minimum Points: ' + str(5))
plt.show()

```

## Descripci√≥n del m√©todo DBSCAN

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de clustering basado en densidad. Fue propuesto por Martin Ester, Hans-Peter Kriegel, J√∂rg Sander y Xiaowei Xu en 1996. El algoritmo tiene como objetivo descubrir clusters de forma arbitraria en un espacio de datos basado en la densidad de los puntos de datos.

### Componentes y Par√°metros Clave

Epsilon (Œµ): Radio m√°ximo de un vecindario de un punto. Este par√°metro define qu√© tan cerca deben estar los puntos para ser considerados vecinos. MinPts: N√∫mero m√≠nimo de puntos que deben estar en un vecindario para que un punto sea considerado un punto central (core point).

![](images/clipboard-2074857476.png)

Un vecindario de un punto se define como la regi√≥n circular con ese punto como el centro y un radio de Epsilon. Cada punto en el conjunto de datos se clasifica en una de las siguientes categor√≠as:

Punto central (Core point): Si hay al menos MinPoints en el vecindario. Punto de borde (Border point): Si este punto se encuentra en el vecindario de un punto central y tiene menos de MinPoints en su propio vecindario. Punto at√≠pico (Outlier point): Tiene menos de MinPoints en su propio vecindario y no tiene ning√∫n otro punto central en su vecindario. Todos los puntos centrales llevan a la formaci√≥n de un cluster. Si dos clusters tienen 2 o m√°s puntos centrales en su vecindario mutuo, esos clusters se fusionan. Despu√©s de este proceso iterativo, nos quedamos con clusters y puntos at√≠picos.

Esta t√©cnica no depende de inicializaciones aleatorias como lo hace K-means, ya que su enfoque se basa en la densidad de los puntos en el espacio de caracter√≠sticas.

![](images/clipboard-1614325023.png)

![](images/clipboard-2717180202.png)

Ejemplo de diferentes distancias:

![](images/clipboard-1426748174.png)

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es √∫til en situaciones del mundo real en las que los datos tienen una distribuci√≥n espacial irregular y pueden contener ruido o valores at√≠picos. Aqu√≠ hay algunos escenarios comunes en los negocios y el mundo real donde DBSCAN puede ser preferible sobre K-means:

Detecci√≥n de anomal√≠as: DBSCAN puede identificar puntos de datos at√≠picos o ruidosos que no se ajustan a ning√∫n grupo particular. Esto es √∫til en la detecci√≥n de fraudes en transacciones financieras, identificaci√≥n de comportamientos inusuales en sistemas de monitoreo de red, o detectar errores en datos de sensores.

Agrupaci√≥n de densidad variable: A diferencia de K-means, DBSCAN puede identificar agrupaciones de diferentes formas y tama√±os, sin necesidad de especificar el n√∫mero de cl√∫steres de antemano. Esto es beneficioso en aplicaciones donde los cl√∫steres pueden tener densidades variables o formas irregulares, como la segmentaci√≥n de clientes en marketing o la agrupaci√≥n de patrones en datos de im√°genes.

Manejo de datos ruidosos: DBSCAN es robusto frente a datos ruidosos y no sensibles a la inicializaci√≥n de los centroides. Esto lo hace adecuado para conjuntos de datos con ruido o valores at√≠picos, donde K-means podr√≠a verse afectado negativamente por la presencia de estos valores.

Eficiencia computacional: DBSCAN tiene una complejidad de tiempo de ejecuci√≥n mejor que K-means para conjuntos de datos grandes, ya que no requiere calcular la distancia entre todos los pares de puntos. Esto lo hace m√°s adecuado para aplicaciones de big data o en entornos donde la eficiencia computacional es crucial.

## K - mediodes

El m√©todo de K-modes es una extensi√≥n del algoritmo K-means, dise√±ado espec√≠ficamente para manejar datos categ√≥ricos. A diferencia de K-means, que se basa en la distancia euclidiana y requiere datos num√©ricos, K-modes utiliza una medida de disimilitud para trabajar con datos categ√≥ricos.

Ventajas de K-modes Manejo de Datos Categ√≥ricos:

K-modes est√° dise√±ado espec√≠ficamente para datos categ√≥ricos, lo que lo hace m√°s adecuado que K-means para este tipo de datos. Simplicidad y Eficiencia:

Similar a K-means, el algoritmo K-modes es simple y eficiente, escalando bien con conjuntos de datos grandes. Interpretabilidad:

Los modos (centroides) resultantes son f√°cilmente interpretables, ya que consisten en valores categ√≥ricos representativos de los cl√∫steres.

```{r}
df_cancer <- read.csv("breast_cancer.csv", header = FALSE) %>% as_tibble()
names(df_cancer) <- c("class", "age", "mefalsepause", "tumor_size", "inv-falsedes",
                      "falsede-caps", "deg_malig", "breast", "breast_quad", "irradiat") 
```

Este conjunto de datos captura diversas caracter√≠sticas demogr√°ficas, patol√≥gicas y relacionadas con el tratamiento de los pacientes con c√°ncer de mama, que pueden ser utilizadas para an√°lisis y potencialmente para predecir resultados de recurrencia.

class (clase) Descripci√≥n: Indica si el paciente experiment√≥ una recurrencia del c√°ncer de mama o no. Valores: "no-recurrence-events" (sin eventos de recurrencia): El paciente no experiment√≥ recurrencia. "recurrence-events" (eventos de recurrencia): El paciente experiment√≥ recurrencia.

age (edad) Descripci√≥n: El rango de edad del paciente. Valores: "30-39": La edad del paciente est√° entre 30 y 39 a√±os. "40-49": La edad del paciente est√° entre 40 y 49 a√±os. "50-59": La edad del paciente est√° entre 50 y 59 a√±os. "60-69": La edad del paciente est√° entre 60 y 69 a√±os. "70-79": La edad del paciente est√° entre 70 y 79 a√±os.

menopause (menopausia): Descripci√≥n: Estado menop√°usico del paciente. Valores: "premeno" (premenop√°usica): El paciente es premenop√°usico. "ge40" (mayor o igual a 40): El paciente tiene 40 a√±os o m√°s y es menop√°usico. "lt40" (menor de 40): El paciente tiene menos de 40 a√±os y es menop√°usico.

tumor_size (tama√±o del tumor): Descripci√≥n: El tama√±o del tumor en mil√≠metros. Valores: Rangos como "0-4", "5-9", "10-14", "15-19", "20-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59".

inv_nodes (nodos invadidos): Descripci√≥n: El n√∫mero de ganglios linf√°ticos axilares positivos detectados (indicativo de propagaci√≥n). Valores: Rangos como "0-2", "3-5", "6-8", "9-11", "12-14", "15-17", "18-20", "21-23", "24-26", "27-29", "30-32", "33-35", "36-39".

node_caps (c√°psula del nodo): Descripci√≥n: Presencia de ganglios linf√°ticos cancerosos m√°s all√° de la c√°psula. Valores: "no": No hay ganglios cancerosos m√°s all√° de la c√°psula. "yes" (s√≠): Hay ganglios cancerosos m√°s all√° de la c√°psula.

deg_malig (grado de malignidad): Descripci√≥n: Grado de malignidad del tumor, que representa cu√°n agresivas son las c√©lulas cancerosas. Valores: 1: Menos agresivo. 2: Moderadamente agresivo. 3: M√°s agresivo.

breast (mama): Descripci√≥n: La mama en la que se detect√≥ el c√°ncer. Valores: "left" (izquierda): El c√°ncer se detect√≥ en la mama izquierda. "right" (derecha): El c√°ncer se detect√≥ en la mama derecha.

breast_quad (cuadrante de la mama): Descripci√≥n: El cuadrante de la mama donde se ubic√≥ el tumor. Valores: "left_up" (superior izquierda): Cuadrante superior izquierdo. "left_low" (inferior izquierda): Cuadrante inferior izquierdo. "right_up" (superior derecha): Cuadrante superior derecho. "right_low" (inferior derecha): Cuadrante inferior derecho. "central" (central): Cuadrante central.

irradiat (irradiaci√≥n): Descripci√≥n: Indica si el paciente recibi√≥ radioterapia. Valores: "no": El paciente no recibi√≥ radioterapia. "yes" (s√≠): El paciente recibi√≥ radioterapia.

```{r}
df_cancer2 <- df_cancer[,2:10]
head(df_cancer2)
```

Utilizamos k-mediodides:

```{r}
k.centers <- kmodes(df_cancer2, 2, iter.max = 100)
k.centers
```

Ejercicio

* EJercicio mushrooms
