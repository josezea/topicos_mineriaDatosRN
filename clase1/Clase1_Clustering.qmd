---
title: "Clustering"
format: html
editor: visual
---

# Prerequisitos
Las siguientes notas se basan en las desarrolladas en a침os anteriores por Eduardo Castro.
-   Anaconda (2024.02-1)
-   R (4.0.0) y Rstudio (2024.04.1+748 \| Released: 2024-05-11)
-   Quarto
-   Visual Studio code con complmentos de Jupyter Notebook
-   Los siguientes paquetes de R se requieren:

```{r, warning=FALSE}
library(dplyr)
library(ggplot2)
library(patchwork)
library(reticulate)
library(cluster)
library(factoextra) # M칠todo silhoute
library(dbscan) # Para m칠todo alternativo de k-means
library(klaR) # Para k mediods

```

# Metodolog칤as para n칰mero de clusters

```{r}
iris_data <- iris[,1:2]
head(iris_data)
```

Graficamos esta nube de puntos:

```{r}
ggplot(data = iris_data, aes(x = Sepal.Length, y = Sepal.Width)) + geom_point() + theme_bw()
```

## M칠todo Silhoute

El algoritmo en R realiza lo siguiente para calcular el coeficiente de silhouette:

-   Para cada observaci칩n en el conjunto de datos, se calcula la distancia media al resto de observaciones dentro de su propio cl칰ster (a) y la distancia media al resto de observaciones en el cl칰ster m치s cercano diferente (b).
-   El coeficiente de silhouette para cada observaci칩n se calcula como (b - a) / max(a, b).
-   El coeficiente de silhouette promedio para el conjunto de datos se calcula como el promedio de los coeficientes de silhouette individuales.

Con este coeficiente podemos encontrar el n칰mero 칩ptimo de clusters:

```{r}
fviz_nbclust(iris_data,kmeans, method = "silhouette",k.max=20)
```

## M칠todo de distancia cuadrada dentro

La suma de los cuadrados dentro del cl칰ster (WSS) es una medida de cu치n cohesivos son los cl칰steres, es decir, cu치n cerca est치n los puntos dentro de un cl칰ster entre s칤. El m칠todo de codo consiste en calcular la WSS para diferentes valores de k (n칰mero de cl칰steres) y graficar estos valores en funci칩n de k. La idea es identificar el "codo" en la curva, que es el punto donde la WSS comienza a disminuir de manera m치s lenta. Este punto indica el n칰mero 칩ptimo de cl칰steres, ya que agregar m치s cl칰steres a partir de este punto no proporciona una mejora significativa en la cohesi칩n dentro de los cl칰steres.

Las f칩rmulas para calcular la WSS son las siguientes:

Para cada cl칰ster $C_i$

Se calcula la distancia cuadrada de cada punto al centroide del cl칰ster $\mu_i$

Se suman estas distancias cuadradas para obtener la WSS del cl칰ster $WSS_i$

$$WSS_i = \sum_{x \in C_i} (x - \mu_i )^2$$
La suma de cuadrados dentro:

```{r}
fviz_nbclust(iris_data,kmeans, method = "wss",k.max=20)

```

## Estad칤stica de brecha

El m칠todo de estad칤sticas de brecha compara la dispersi칩n intra-cl칰ster de un conjunto de datos con la que se esperar칤a si los datos fueran generados aleatoriamente sin estructura de cl칰steres. La idea es que si los datos tienen una estructura de cl칰steres significativa, la dispersi칩n intra-cl칰ster ser치 mayor que en datos aleatorios.

El m칠todo de estad칤sticas de brecha se calcula en tres pasos:

- Se ajustan modelos de clustering para diferentes valores de k (n칰mero de cl칰steres).
- Se calcula la estad칤stica de brecha para cada valor de 洧녲
k, que mide la diferencia entre la dispersi칩n intra-cl칰ster observada y la esperada bajo una referencia nula (datos aleatorios).
- Se selecciona el valor de洧녲k que maximiza la estad칤stica de brecha.


Dispersi칩n Intra-Cl칰ster Observada ($W_k$: Se calcula la suma de los cuadrados dentro del cl칰ster para el valor de k. Esta medida representa qu칠 tan compactos son los cl칰steres en los datos reales.

Dispersi칩n Intra-Cl칰ster Esperada ($W_{kb}$): Se calcula la dispersi칩n intra-cl칰ster para un conjunto de datos aleatorios con el mismo n칰mero de puntos y la misma distribuci칩n de caracter칤sticas que los datos originales. Este c치lculo se realiza promediando la dispersi칩n intra-cl칰ster sobre un cierto n칰mero de iteraciones.

Estad칤stica de Brecha ($GAP_k$): La estad칤stica de brecha para el valor de k dado se calcula como la diferencia entre la dispersi칩n intra-cl칰ster observada y la esperada bajo la referencia nula:

$$GAP_k = \frac{1}{B}\sum_{b=1}^{B}log(W_{kb})-log(W_k)$$

```{r}
fviz_nbclust(iris_data,kmeans, method = "gap_stat",k.max=20)
```

# M칠todos alternativos de clustering 


No todos los problemas se pueden resolver con kmeans, sino que existen t칠cnicas m치s apropiadas para detectar de forma m치s efectiva como se compartan los puntos en un conjunto de datos:

```{r}
df_shapes <- read.csv("multishapes.csv") %>% dplyr::select(x, y)
```

Calculamos por k - meadias:

```{r}
set.seed(12345)
kmedias <- kmeans(df_shapes, 5)
df_shapes$grupo_kmedias <- kmedias$cluster %>% as.character()
```


## DBSCAN  (Density-Based Spatial Clustering of Applications with Noise)

AHora calculamos por el m칠todo debscan:

```{r}
dbscan_method <- dbscan(df_shapes %>% dplyr::select(x, y), eps = .15, minPts = 5)
df_shapes$grupo_dmscan <- dbscan_method$cluster %>% as.character()
```

```{r}
table(df_shapes$grupo_dmscan)
```

```{r}

grafico_kmeans <- ggplot(data = df_shapes, aes(x, y, color = grupo_kmedias)) + geom_point()
grafico_dmscan <- ggplot(data = df_shapes, aes(x, y, color = grupo_dmscan)) + geom_point()
grafico_kmeans + grafico_dmscan
```

En Python podemos resolver de la siguiente manera:

Cargamos los paquetes:

```{python}
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.cluster import DBSCAN
from collections import Counter
from sklearn.datasets import make_blobs
import matplotlib.pyplot as plt
```

Traemos unos datos para desarrollar elm칠todo debscan en Python:

```{python}

# Read the CSV file
df_shapes = pd.read_csv("multishapes.csv")

# Select columns 'x' and 'y'
df_shapes = df_shapes[['x', 'y']]
```

Calculamos el m칠todo dbscan:

```{python}
dbscan_method = DBSCAN(eps = 0.15, min_samples = 5, metric = 'euclidean')
plt.scatter(df_shapes.iloc[:,0], df_shapes.iloc[:,1], c = dbscan_method.fit_predict(df_shapes))
plt.title('Toy Problem with Minimum Points: ' + str(5))
plt.show()
```

EL -1 hace referencia al grupo de outliers:

```{python}
labels = dbscan_method.fit_predict(df_shapes)
Counter(labels)
```

Ilustraremos el m칠todo para diferentes valores de epsilon:

```{python}
XX, yy = make_blobs(n_samples=1000, centers=8, n_features=2, random_state=800)
# Visualize the data
plt.scatter(XX[:,0], XX[:,1])
plt.show()
```

```{python, warning = FALSE, message = FALSE}
num_samples = [10,19,20]

for min_num in num_samples:
    db = DBSCAN(eps=0.7, min_samples=min_num, metric='euclidean')
    plt.scatter(XX[:,0], XX[:,1], c=db.fit_predict(XX))
    plt.title('Toy Problem with Minimum Points: ' + str(min_num))
    plt.show()
```

Observemos como var칤a el radio:

```{python}
epsilons = [0.4,0.7]

for epsilon in epsilons:
    db = DBSCAN(eps=epsilon, min_samples=10, metric='euclidean')
    plt.scatter(XX[:,0], XX[:,1], c=db.fit_predict(XX))
    plt.title('Toy Problem with Minimum Points: ' + str(epsilon))
    plt.show()
```

Podemos tambi칠n entrenar el modelo para cuando lleguen nuevos datos:

```{python}
# Assuming your data has features 'x' and 'y', select those columns

# Split the data into training and testing sets
X_train, X_test = train_test_split(df_shapes, test_size=0.2, random_state=12345)

# Initialize DBSCAN model
dbscan = DBSCAN(eps=0.15, min_samples=5)

# Train the model
dbscan.fit(X_train)

# Predict cluster labels for the test data
test_labels = dbscan.fit_predict(X_test)
print(test_labels)
```

```{python}
plt.scatter(X_test.iloc[:,0], X_test.iloc[:,1],
            c = dbscan_method.fit_predict(X_test))
plt.title('Toy Problem with Minimum Points: ' + str(5))
plt.show()

```

## Descripci칩n del m칠todo DBSCAN

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es un algoritmo de clustering basado en densidad. Fue propuesto por Martin Ester, Hans-Peter Kriegel, J칬rg Sander y Xiaowei Xu en 1996. El algoritmo tiene como objetivo descubrir clusters de forma arbitraria en un espacio de datos basado en la densidad de los puntos de datos.

### Componentes y Par치metros Clave

Epsilon (풧): Radio m치ximo de un vecindario de un punto. Este par치metro define qu칠 tan cerca deben estar los puntos para ser considerados vecinos. MinPts: N칰mero m칤nimo de puntos que deben estar en un vecindario para que un punto sea considerado un punto central (core point).

![](images/clipboard-2074857476.png)

Un vecindario de un punto se define como la regi칩n circular con ese punto como el centro y un radio de Epsilon. Cada punto en el conjunto de datos se clasifica en una de las siguientes categor칤as:

Punto central (Core point): Si hay al menos MinPoints en el vecindario. Punto de borde (Border point): Si este punto se encuentra en el vecindario de un punto central y tiene menos de MinPoints en su propio vecindario. Punto at칤pico (Outlier point): Tiene menos de MinPoints en su propio vecindario y no tiene ning칰n otro punto central en su vecindario. Todos los puntos centrales llevan a la formaci칩n de un cluster. Si dos clusters tienen 2 o m치s puntos centrales en su vecindario mutuo, esos clusters se fusionan. Despu칠s de este proceso iterativo, nos quedamos con clusters y puntos at칤picos.

Esta t칠cnica no depende de inicializaciones aleatorias como lo hace K-means, ya que su enfoque se basa en la densidad de los puntos en el espacio de caracter칤sticas.

![](images/clipboard-1614325023.png)

![](images/clipboard-2717180202.png)

Ejemplo de diferentes distancias:

![](images/clipboard-1426748174.png)

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) es 칰til en situaciones del mundo real en las que los datos tienen una distribuci칩n espacial irregular y pueden contener ruido o valores at칤picos. Aqu칤 hay algunos escenarios comunes en los negocios y el mundo real donde DBSCAN puede ser preferible sobre K-means:

Detecci칩n de anomal칤as: DBSCAN puede identificar puntos de datos at칤picos o ruidosos que no se ajustan a ning칰n grupo particular. Esto es 칰til en la detecci칩n de fraudes en transacciones financieras, identificaci칩n de comportamientos inusuales en sistemas de monitoreo de red, o detectar errores en datos de sensores.

Agrupaci칩n de densidad variable: A diferencia de K-means, DBSCAN puede identificar agrupaciones de diferentes formas y tama침os, sin necesidad de especificar el n칰mero de cl칰steres de antemano. Esto es beneficioso en aplicaciones donde los cl칰steres pueden tener densidades variables o formas irregulares, como la segmentaci칩n de clientes en marketing o la agrupaci칩n de patrones en datos de im치genes.

Manejo de datos ruidosos: DBSCAN es robusto frente a datos ruidosos y no sensibles a la inicializaci칩n de los centroides. Esto lo hace adecuado para conjuntos de datos con ruido o valores at칤picos, donde K-means podr칤a verse afectado negativamente por la presencia de estos valores.

Eficiencia computacional: DBSCAN tiene una complejidad de tiempo de ejecuci칩n mejor que K-means para conjuntos de datos grandes, ya que no requiere calcular la distancia entre todos los pares de puntos. Esto lo hace m치s adecuado para aplicaciones de big data o en entornos donde la eficiencia computacional es crucial.

## K - mediodes

El m칠todo de K-modes es una extensi칩n del algoritmo K-means, dise침ado espec칤ficamente para manejar datos categ칩ricos. A diferencia de K-means, que se basa en la distancia euclidiana y requiere datos num칠ricos, K-modes utiliza una medida de disimilitud para trabajar con datos categ칩ricos.

Ventajas de K-modes Manejo de Datos Categ칩ricos:

K-modes est치 dise침ado espec칤ficamente para datos categ칩ricos, lo que lo hace m치s adecuado que K-means para este tipo de datos. Simplicidad y Eficiencia:

Similar a K-means, el algoritmo K-modes es simple y eficiente, escalando bien con conjuntos de datos grandes. Interpretabilidad:

Los modos (centroides) resultantes son f치cilmente interpretables, ya que consisten en valores categ칩ricos representativos de los cl칰steres.

```{r}
df_cancer <- read.csv("breast_cancer.csv", header = FALSE) %>% as_tibble()
names(df_cancer) <- c("class", "age", "mefalsepause", "tumor_size", "inv-falsedes",
                      "falsede-caps", "deg_malig", "breast", "breast_quad", "irradiat") 
```

Este conjunto de datos captura diversas caracter칤sticas demogr치ficas, patol칩gicas y relacionadas con el tratamiento de los pacientes con c치ncer de mama, que pueden ser utilizadas para an치lisis y potencialmente para predecir resultados de recurrencia.

class (clase) Descripci칩n: Indica si el paciente experiment칩 una recurrencia del c치ncer de mama o no. Valores: "no-recurrence-events" (sin eventos de recurrencia): El paciente no experiment칩 recurrencia. "recurrence-events" (eventos de recurrencia): El paciente experiment칩 recurrencia.

age (edad) Descripci칩n: El rango de edad del paciente. Valores: "30-39": La edad del paciente est치 entre 30 y 39 a침os. "40-49": La edad del paciente est치 entre 40 y 49 a침os. "50-59": La edad del paciente est치 entre 50 y 59 a침os. "60-69": La edad del paciente est치 entre 60 y 69 a침os. "70-79": La edad del paciente est치 entre 70 y 79 a침os.

menopause (menopausia): Descripci칩n: Estado menop치usico del paciente. Valores: "premeno" (premenop치usica): El paciente es premenop치usico. "ge40" (mayor o igual a 40): El paciente tiene 40 a침os o m치s y es menop치usico. "lt40" (menor de 40): El paciente tiene menos de 40 a침os y es menop치usico.

tumor_size (tama침o del tumor): Descripci칩n: El tama침o del tumor en mil칤metros. Valores: Rangos como "0-4", "5-9", "10-14", "15-19", "20-24", "25-29", "30-34", "35-39", "40-44", "45-49", "50-54", "55-59".

inv_nodes (nodos invadidos): Descripci칩n: El n칰mero de ganglios linf치ticos axilares positivos detectados (indicativo de propagaci칩n). Valores: Rangos como "0-2", "3-5", "6-8", "9-11", "12-14", "15-17", "18-20", "21-23", "24-26", "27-29", "30-32", "33-35", "36-39".

node_caps (c치psula del nodo): Descripci칩n: Presencia de ganglios linf치ticos cancerosos m치s all치 de la c치psula. Valores: "no": No hay ganglios cancerosos m치s all치 de la c치psula. "yes" (s칤): Hay ganglios cancerosos m치s all치 de la c치psula.

deg_malig (grado de malignidad): Descripci칩n: Grado de malignidad del tumor, que representa cu치n agresivas son las c칠lulas cancerosas. Valores: 1: Menos agresivo. 2: Moderadamente agresivo. 3: M치s agresivo.

breast (mama): Descripci칩n: La mama en la que se detect칩 el c치ncer. Valores: "left" (izquierda): El c치ncer se detect칩 en la mama izquierda. "right" (derecha): El c치ncer se detect칩 en la mama derecha.

breast_quad (cuadrante de la mama): Descripci칩n: El cuadrante de la mama donde se ubic칩 el tumor. Valores: "left_up" (superior izquierda): Cuadrante superior izquierdo. "left_low" (inferior izquierda): Cuadrante inferior izquierdo. "right_up" (superior derecha): Cuadrante superior derecho. "right_low" (inferior derecha): Cuadrante inferior derecho. "central" (central): Cuadrante central.

irradiat (irradiaci칩n): Descripci칩n: Indica si el paciente recibi칩 radioterapia. Valores: "no": El paciente no recibi칩 radioterapia. "yes" (s칤): El paciente recibi칩 radioterapia.

```{r}
df_cancer2 <- df_cancer[,2:10]
head(df_cancer2)
```

Utilizamos k-mediodides:

```{r}
k.centers <- kmodes(df_cancer2, 2, iter.max = 100)
k.centers
```

Ejercicio

* EJercicio mushrooms
